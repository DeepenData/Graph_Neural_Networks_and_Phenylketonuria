{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
      "Problem data seem to be well scaled\n"
     ]
    }
   ],
   "source": [
    "from custom_clss_and_fncs import *\n",
    "from cobra.io import load_json_model\n",
    "\n",
    "model                                 = load_json_model(\"./COBRA_models/GEM_Recon2_thermocurated_redHUMAN.json\")\n",
    "feature_data = pd.read_parquet(\"./results/data/oversampled_augmented_metabolite_data.parquet.gzip\")#\n",
    "feature_names                         = pd.read_csv(\"./results/data/metabolite_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add concentratios as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo_nx                              = cobra_a_networkx(model)\n",
    "\n",
    "node_list = list(grafo_nx.nodes)\n",
    "feature_data.rename(\n",
    "    columns=feature_names.set_index(\"Simbolo_traductor\")[\"Recon3_ID\"].to_dict(), \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "number_to_fill_in = int(1)\n",
    "\n",
    "feature_data[\n",
    "    [item for item in node_list if item not in feature_data.columns.tolist()]\n",
    "] = number_to_fill_in\n",
    "\n",
    "# Re-empaca todo este vector como un diccionario de atributos\n",
    "feature_dict = feature_data.to_dict(orient=\"list\")  # desde Pandas\n",
    "feature_dict = {key: {\"x\": feature_dict[key]} for key in feature_dict}\n",
    "\n",
    "nx.set_node_attributes(grafo_nx, feature_dict)\n",
    "\n",
    "assert feature_data['phe_L_c'].tolist() == grafo_nx.nodes['phe_L_c']['x']\n",
    "assert np.unique(grafo_nx.nodes(data=True)['r0399']['x']) == number_to_fill_in\n",
    "assert len(grafo_nx.nodes(data=True)['r0399']['x']) == len(grafo_nx.nodes(data=True)['phe_L_c']['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph       = from_networkx(copy.deepcopy(nx.Graph(grafo_nx)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attribute     = nx.get_node_attributes(grafo_nx, \"x\")\n",
    "longest_feature = max(len(v) for k,v in x_attribute.items())\n",
    "assert pyg_graph.x.shape[1]  == pyg_graph.num_features == longest_feature # == flux_samples.shape[0]\n",
    "target_node = 'phe_L_c'\n",
    "producto_idx = list(grafo_nx.nodes()).index(target_node)\n",
    "producto_features = grafo_nx.nodes()[target_node]['x']\n",
    "\n",
    "\n",
    "#assert grafo_nx.nodes[target_node]['x'] == flux_dict[target_node]\n",
    "assert np.allclose(pyg_graph.x[producto_idx,:].numpy()[0:producto_features.__len__()], np.array(producto_features), 1e-7, 1e-10)\n",
    "#assert np.allclose(flux_samples[target_node].tolist(), pyg_graph.x[producto_idx,:].numpy()[0:producto_features.__len__()], 1e-7, 1e-10)\n",
    "assert not pyg_graph.is_directed()\n",
    "assert not pyg_graph.has_isolated_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_metabolite_data         = pd.read_parquet(\"./results/data/oversampled_augmented_metabolite_data.parquet.gzip\")#\n",
    "labels                              = torch.tensor(oversampled_metabolite_data.label).reshape(len(oversampled_metabolite_data.label),1)\n",
    "pyg_graph.y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(pyg_graph, \"./results/graphs_from_PYG_and_NX/PYG_graph_only_CONCEN.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "producto_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "lista_de_grafos = []\n",
    "for i in range(pyg_graph.x.shape[1]):\n",
    "   \n",
    "    \n",
    "    new_pyg_data  = Data(x =  pyg_graph.x[:,i].reshape(12437, 1),  y = pyg_graph.y[i], \n",
    "                         edge_index = pyg_graph.edge_index)\n",
    "    new_pyg_data.num_classes = int(2)\n",
    "    lista_de_grafos.append(new_pyg_data)\n",
    "    \n",
    "    \n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def make_loader(graphs: list, batch_size: int  =1*32, num_samples = 500):#batch_size: int  =8*32):\n",
    "\n",
    "    sampler_train_set = RandomSampler(\n",
    "        graphs,\n",
    "        #num_samples= num_samples, #params[\"training\"][\"sampler_num_samples\"],  # Genera un muestreo del grafo\n",
    "        replacement=True,  # con repeticion de muestras\n",
    "    )\n",
    "    return DataLoader(graphs, batch_size=batch_size, sampler = sampler_train_set,  drop_last=True)\n",
    "\n",
    "\n",
    "loader   = make_loader(lista_de_grafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.9550, Test Acc: 0.9634\n",
      "Epoch: 002, Train Acc: 0.8896, Test Acc: 0.8888\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GIN\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.GIN_layers =  GIN(in_channels= 1, hidden_channels= hidden_channels, num_layers= 4, \n",
    "                               out_channels= hidden_channels, dropout=0.1,  jk=None, \n",
    "                               act='LeakyReLU', act_first = True)   \n",
    "        \n",
    "        #self.conv1 = GCNConv(1, hidden_channels)\n",
    "        #self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        #self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 2, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        #x = self.conv1(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        x = self.GIN_layers(x, edge_index)\n",
    "        #x = F.dropout(x, p=0.1)\n",
    "        x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) #x\n",
    "    \n",
    "model    = GCN(hidden_channels=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "def train():\n",
    "    model.to('cuda')\n",
    "    model.train()\n",
    "\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "         data.to('cuda')\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     model.to('cuda')\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data.to('cuda')\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    train_acc = test(loader)\n",
    "    test_acc = test(loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(grafo_nx, f\"./results/graphs_from_PYG_and_NX/NX_only_concentrations.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo_nx_v2 = copy.deepcopy(grafo_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add fluxes as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_samples = pd.read_parquet( \"./results/data/flux_samples.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_length = len(grafo_nx.nodes(data=True)['r0399']['x']) \n",
    "\n",
    "\n",
    "flux_dict = flux_samples.sample(feature_length, replace=True).to_dict(orient = 'list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attribute     = nx.get_node_attributes(grafo_nx_v2, \"x\")\n",
    "\n",
    "x_attribute.update(flux_dict)\n",
    "\n",
    "nx.set_node_attributes(grafo_nx_v2, x_attribute, 'x')\n",
    "\n",
    "\n",
    "assert grafo_nx_v2.nodes(data=True)['r0399']['x'] == flux_dict['r0399']\n",
    "assert len(grafo_nx_v2.nodes(data=True)['r0399']['x']) == len(grafo_nx_v2.nodes(data=True)['phe_L_c']['x'])\n",
    "assert grafo_nx_v2.nodes(data=True)['phe_L_c']['x']  == feature_data['phe_L_c'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(grafo_nx_v2, f\"./results/graphs_from_PYG_and_NX/NX_fluxes_and_concentrations.gpickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ccb92c3caf64d15d8cccade25008a463e602c087926676408820d80b4769698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
