{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No defined compartments in model model. Compartments will be deduced heuristically using regular expressions.\n",
      "Using regular expression found the following compartments:c, e, g, i, l, m, n, r, x\n"
     ]
    }
   ],
   "source": [
    "from    acevedo_clss_and_fcns import * \n",
    "from    cobra.io.mat import *\n",
    "\n",
    "model               = load_matlab_model(\"./COBRA_models/GEM_Recon3_thermocurated_redHUMAN_AA.mat\")\n",
    "flux_samples_CONTROL_7_000  = pd.read_parquet(\"./results/fluxes/flux_samples_CONTROL_cleaned.parquet.gzip\")#.abs()\n",
    "flux_samples_PKU_7_000      = pd.read_parquet(\"./results/fluxes/flux_samples_PKU_cleaned.parquet.gzip\")#.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "concentration_data = pd.read_parquet(\"./results/dataframes/concentrations/augmented_balanced_metabolite_data.parquet.gzip\").abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_names       = pd.read_csv(\"./metabolites_data/metabolite_names.csv\")\n",
    "grafo_nx            = cobra_to_networkx(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration_data['Leu'] = concentration_data['Leu.Ile']\n",
    "concentration_data['Ile'] = concentration_data['Leu.Ile']\n",
    "concentration_data.drop('Leu.Ile', axis=1, inplace=True)\n",
    "\n",
    "concentration_data['C3DC'] = concentration_data['C4OH.C3DC']\n",
    "concentration_data['C4OH'] = concentration_data['C4OH.C3DC']\n",
    "concentration_data.drop('C4OH.C3DC', axis=1, inplace=True)\n",
    "\n",
    "concentration_data['C4DC'] = concentration_data['C5.OH.C4DC']\n",
    "concentration_data['C5OH'] = concentration_data['C5.OH.C4DC']\n",
    "concentration_data.drop('C5.OH.C4DC', axis=1, inplace=True)\n",
    "assert len(set(feature_names.Simbolo_traductor) - set(concentration_data.columns)) == 0\n",
    "\n",
    "\n",
    "concentration_data.rename(\n",
    "        columns=feature_names.set_index(\"Simbolo_traductor\")[\"Recon3_ID\"].to_dict(), \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "assert set(set(concentration_data.columns)-set([\"label\"])).issubset(set(list(grafo_nx.nodes)))\n",
    "\n",
    "w  = dict(zip(grafo_nx.edges() , itertools.repeat(1)))\n",
    "\n",
    "nx.set_edge_attributes(grafo_nx, w, \"weight\")\n",
    "\n",
    "assert 1 == np.unique(list(nx.get_edge_attributes(grafo_nx, \"weight\").values())).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_subset(full_samples, concentration_data, label): \n",
    "\n",
    "\n",
    "    s = concentration_data.label.value_counts()\n",
    "    sample_subset = full_samples.sample(s.loc[label], replace=True).reset_index(drop=True)\n",
    "    sample_subset[\"label\"] = label\n",
    "    \n",
    "    return sample_subset\n",
    "\n",
    "\n",
    "\n",
    "flux_samples_CONTROL = get_sample_subset(flux_samples_CONTROL_7_000, concentration_data, 0)\n",
    "flux_samples_PKU     = get_sample_subset(flux_samples_PKU_7_000,     concentration_data, 1)\n",
    "assert flux_samples_CONTROL.r0399.max() > 20 \n",
    "assert flux_samples_PKU.r0399.max() < 4\n",
    "assert all(flux_samples_CONTROL.columns == flux_samples_PKU.columns)\n",
    "\n",
    "flux_samples_CONTROL = flux_samples_CONTROL.reindex(columns=flux_samples_PKU.columns)\n",
    "flux_samples         = pd.concat([flux_samples_CONTROL, flux_samples_PKU], axis=0)\n",
    "flux_samples         = flux_samples.reset_index(drop=True, inplace=False)\n",
    "\n",
    "assert len(flux_samples.columns) == len(flux_samples_CONTROL.columns)\n",
    "assert len(concentration_data) == len(flux_samples)\n",
    "assert flux_samples.r0399.loc[flux_samples.label == 0].mean() > 20\n",
    "assert flux_samples.r0399.loc[flux_samples.label == 1].mean() < 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_cc(G):\n",
    "    \n",
    "  largest_wcc = max(nx.connected_components(nx.Graph(G)), key=len)\n",
    "\n",
    "\n",
    "  # Create a subgraph SG based on G\n",
    "  SG = G.__class__()\n",
    "  SG.add_nodes_from((n, G.nodes[n]) for n in largest_wcc)\n",
    "\n",
    "\n",
    "  SG.add_edges_from((n, nbr, d)\n",
    "      for n, nbrs in G.adj.items() if n in largest_wcc\n",
    "      for nbr, d in nbrs.items() if nbr in largest_wcc)\n",
    "\n",
    "  SG.graph.update(G.graph)\n",
    "\n",
    "  assert G.nodes.__len__() >= SG.nodes.__len__()\n",
    "  assert G.edges.__len__() >= SG.edges.__len__()\n",
    "  assert SG.nodes.__len__() == len(largest_wcc)\n",
    "  assert not SG.is_directed() \n",
    "  assert nx.is_connected(nx.Graph(SG))\n",
    "\n",
    "  return copy.deepcopy(SG)\n",
    "\n",
    "\n",
    "mets = ['nad_', 'nadh_', \"nadp_\", \"nadph_\", \"adp_\", \"atp_\", \"gdt_\", \"gtp_\",\n",
    "        \"pi_\", \"ppi_\", \"pppi_\", \"co2_\", \"hco3_\", \"h2o_\", \"h2o2_\", \"h_\", \"o2_\", \"oh1_\",\n",
    "        \"o2s_\", \"fad_\",  \"fadh2_\", \"nh4_\", \"so3_\", \"so4_\", \"cl_\", \"k_\", \"na1_\",\n",
    "        \"i_\", \"fe2_\", \"fe3_\", \"mg2_\", \"ca2\", \"zn2_\", \"M02382_\"]\n",
    "\n",
    "to_remove = []\n",
    "for m in mets:\n",
    "\n",
    "    to_remove.extend([\"\".join(l) for l in list(zip(itertools.repeat(m), list(model.compartments.keys())))])\n",
    "    \n",
    "grafo_nx.remove_nodes_from(to_remove)\n",
    "\n",
    "grafo_nx = get_largest_cc(grafo_nx)\n",
    "    \n",
    "nx.write_gpickle(grafo_nx, \"./results/graphs/NX_recon_graph.gpickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration_data = concentration_data.sort_values(by=['label'])\n",
    "concentration_data.reset_index(drop=True, inplace=True)\n",
    "flux_samples = flux_samples.sort_values(by=['label'])\n",
    "flux_samples.reset_index(drop=True, inplace=True)\n",
    "assert all(concentration_data.label == flux_samples.label)\n",
    "\n",
    "Labels = flux_samples.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flux_samples.to_parquet(\"./results/dataframes/fluxes/all_flux_samples.parquet.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flux_samples.drop(\"label\", axis=1, inplace=True)\n",
    "concentration_data.drop(\"label\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler             = MinMaxScaler(feature_range=(0, 1))\n",
    "concentration_data_array = scaler.fit_transform(concentration_data.drop( [\"phe_L_c\",\"tyr_L_c\"], axis=1))\n",
    "concentration_data_scaled       = pd.DataFrame(concentration_data_array, columns=concentration_data.drop([\"phe_L_c\",\"tyr_L_c\"], axis=1).columns)\n",
    "concentration_data_scaled[\"phe_L_c\"]   = concentration_data.phe_L_c\n",
    "concentration_data_scaled[\"tyr_L_c\"]   = concentration_data.tyr_L_c\n",
    "\n",
    "\n",
    "\n",
    "scaler             = MinMaxScaler(feature_range=(0, 1))\n",
    "flux_samples_array = scaler.fit_transform(flux_samples.drop( [\"r0399\",\"PHETHPTOX2\"], axis=1))\n",
    "flux_samples_scaled       = pd.DataFrame(flux_samples_array, columns=flux_samples.drop([\"r0399\",\"PHETHPTOX2\"], axis=1).columns)\n",
    "flux_samples_scaled[\"r0399\"]   = flux_samples.r0399\n",
    "flux_samples_scaled[\"PHETHPTOX2\"]   = flux_samples.PHETHPTOX2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_features = pd.DataFrame(\n",
    "                    np.full((len(concentration_data_scaled), list(grafo_nx.nodes).__len__()), 1e-10),  columns=list(grafo_nx.nodes)\n",
    "                    )\n",
    "blank_features.reset_index(drop=True, inplace=True)\n",
    "assert len(blank_features) == len(concentration_data_scaled) == len(flux_samples_scaled)\n",
    "assert set(concentration_data_scaled.columns).issubset(set(list(grafo_nx.nodes)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_features_dict = blank_features.to_dict(orient=\"list\")  \n",
    "flux_samples_dict   = flux_samples_scaled.to_dict(orient=\"list\")  \n",
    "concentrations_dict = concentration_data_scaled.to_dict(orient=\"list\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def update_df_features(base: dict, new:dict):\n",
    "    \n",
    "    common_vars       = list(set(base.keys()).intersection(set(new)))\n",
    "    base_updated      = copy.deepcopy(base)\n",
    "    base_updated.update({key: new[key] for key in common_vars})\n",
    "    \n",
    "    return base_updated\n",
    "\n",
    "\n",
    "features_only_concentrations_dict = update_df_features(blank_features_dict, concentrations_dict)\n",
    "features_only_fluxes_dict         = update_df_features(blank_features_dict, flux_samples_dict)\n",
    "\n",
    "full_features_dict = copy.deepcopy(concentrations_dict)\n",
    "full_features_dict.update(flux_samples_dict)  # \n",
    "features_completed_dict = update_df_features(blank_features_dict, full_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_nx_from_dict(nx_G_in, feature_dict):\n",
    "    \n",
    "    nx_G        = copy.deepcopy(nx_G_in)    \n",
    "    x_attribute = feature_dict #nx.get_node_attributes(nx_G, \"x\")\n",
    "     \n",
    "    nx.set_node_attributes(nx_G, x_attribute, 'x')\n",
    "    \n",
    "    len(nx_G.nodes(data=True)['r0399']['x']) == len(nx_G.nodes(data=True)['phe_L_c']['x'])\n",
    "    assert nx_G.nodes(data=True)['phe_L_c']['x']  == feature_dict['phe_L_c']#.tolist() \n",
    "    assert nx_G.nodes(data=True)['r0399']['x']  == feature_dict['r0399']#.tolist() \n",
    "\n",
    "    \n",
    "    \n",
    "    return nx_G\n",
    "\n",
    "\n",
    "nx_features_only_concentrations = new_nx_from_dict(grafo_nx, features_only_concentrations_dict)  \n",
    "\n",
    "conc_df = pd.DataFrame(\n",
    "nx.get_node_attributes(nx_features_only_concentrations, 'x'))\n",
    "\n",
    "assert set(conc_df.sum().loc[lambda x: abs(x)>=2e-6].index.tolist()) ==  set([k for k in concentrations_dict])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nx_features_only_fluxes         = new_nx_from_dict(grafo_nx, features_only_fluxes_dict)  \n",
    "\n",
    "flux_df = pd.DataFrame(\n",
    "nx.get_node_attributes(nx_features_only_fluxes, 'x'))\n",
    "\n",
    "rxn_list_recon: list[str] = [model.reactions[i].id       for i in range(model.reactions.__len__())]\n",
    "met_list_recon: list[str] = [model.metabolites[i].id     for i in range(model.metabolites.__len__())]\n",
    "first_partition , second_partition = bipartite.sets(grafo_nx)\n",
    "\n",
    "if first_partition.__len__() > second_partition.__len__():\n",
    "    rxn_partition = first_partition\n",
    "    met_partition = second_partition\n",
    "else:\n",
    "    rxn_partition = second_partition \n",
    "    met_partition = first_partition\n",
    "    \n",
    "assert set(rxn_partition).issubset(set(rxn_list_recon)) and set(met_partition).issubset(set(met_list_recon))\n",
    "assert len(set(rxn_partition) - set(rxn_list_recon)) == 0\n",
    "assert len(set(met_partition) - set(met_list_recon)) == 0\n",
    "\n",
    "partition_list =  np.array(list(nx.get_node_attributes(grafo_nx, \"bipartite\").values()))\n",
    "mask_rxns      =  partition_list.astype(bool)\n",
    "mask_mets      =  np.invert(partition_list.astype(bool))\n",
    "assert flux_df.loc[:,mask_mets].sum().unique().__len__() == 1\n",
    "assert flux_df.loc[:,mask_rxns].sum().unique().__len__() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_full_features_completed  = new_nx_from_dict(grafo_nx, features_completed_dict)  \n",
    "full_features_df = pd.DataFrame(nx.get_node_attributes(nx_full_features_completed, 'x'))\n",
    "\n",
    "assert set(full_features_df.loc[:,mask_mets].sum().loc[lambda x: abs(x)>=2e-6].index.tolist()) ==  set([k for k in concentrations_dict])\n",
    "assert full_features_df.loc[:,mask_mets].sum().loc[lambda x:  abs(x)<2e-6].unique().__len__() == 1\n",
    "assert full_features_df.loc[:,mask_rxns].sum().unique().__len__() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nx_features_only_concentrations.nodes(data=True)['r0399']['x']    != nx_features_only_fluxes.nodes(data=True)['r0399']['x'] \n",
    "assert nx_features_only_concentrations.nodes(data=True)['phe_L_c']['x']  != nx_features_only_fluxes.nodes(data=True)['phe_L_c']['x'] \n",
    "assert nx_full_features_completed.nodes(data=True)['phe_L_c']['x']  == nx_features_only_concentrations.nodes(data=True)['phe_L_c']['x'] \n",
    "assert nx_full_features_completed.nodes(data=True)['r0399']['x']    == nx_features_only_fluxes.nodes(data=True)['r0399']['x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph_onlyConcen         = make_PYG_graph_from_grafo_nx(nx_features_only_concentrations)\n",
    "pyg_graph_onlyFluxes         = make_PYG_graph_from_grafo_nx(nx_features_only_fluxes)\n",
    "pyg_graph_Concen_plus_Fluxes = make_PYG_graph_from_grafo_nx(nx_full_features_completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_list = list(grafo_nx.nodes)\n",
    "\n",
    "assert pyg_graph_onlyConcen.x.shape[0] == len(node_list) == len(nx_full_features_completed.nodes) == len(nx_features_only_fluxes.nodes)== len(nx_features_only_concentrations.nodes)\n",
    "assert pyg_graph_onlyFluxes.x.shape[0] == len(node_list) #== len(nx_full_features_completed.nodes) == len(nx_features_only_fluxes.nodes)== len(nx_features_only_concentrations.nodes)\n",
    "assert pyg_graph_Concen_plus_Fluxes.x.shape[0] == len(node_list) #== len(nx_full_features_completed.nodes) == len(nx_features_only_fluxes.nodes)== len(nx_features_only_concentrations.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph_onlyConcen.y               = torch.tensor(Labels).reshape(len(Labels),1)\n",
    "pyg_graph_onlyFluxes.y               = torch.tensor(Labels).reshape(len(Labels),1)\n",
    "pyg_graph_Concen_plus_Fluxes.y       = torch.tensor(Labels).reshape(len(Labels),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "assert set(compress(compress(node_list, mask_mets), pyg_graph_onlyConcen.x.numpy()[mask_mets,:].sum(axis=1)>=2e-6)) == set([k for k in concentrations_dict])\n",
    "assert np.unique(\n",
    "                pyg_graph_onlyConcen.x.numpy()[mask_mets,:].sum(axis=1)[\n",
    "                np.invert(pyg_graph_onlyConcen.x.numpy()[mask_mets,:].sum(axis=1)>=2e-6)\n",
    "                ]).__len__() == 1\n",
    "\n",
    "assert np.unique(\n",
    "                pyg_graph_onlyConcen.x.numpy()[mask_rxns,:].sum(axis=1)\n",
    "                ).__len__() == 1\n",
    "\n",
    "assert np.unique(\n",
    "                pyg_graph_onlyFluxes.x.numpy()[mask_mets,:].sum(axis=1)\n",
    "                ).__len__() == 1\n",
    "\n",
    "assert np.unique(\n",
    "                pyg_graph_onlyFluxes.x.numpy()[mask_rxns,:].sum(axis=1)\n",
    "                ).__len__() > 1\n",
    "#pyg_graph_Concen_plus_Fluxes\n",
    "assert np.unique(\n",
    "                pyg_graph_Concen_plus_Fluxes.x.numpy()[mask_mets,:].sum(axis=1)[\n",
    "                                                                                np.invert(\n",
    "                                                                                    pyg_graph_Concen_plus_Fluxes.x.numpy()[mask_mets,:].sum(axis=1)  >=2e-6   \n",
    "                                                                                )\n",
    "                                                                                ]).__len__() ==1\n",
    "\n",
    "\n",
    "assert np.unique(\n",
    "                pyg_graph_Concen_plus_Fluxes.x.numpy()[mask_mets,:].sum(axis=1)[\n",
    "                                                                                #np.invert(\n",
    "                                                                                    pyg_graph_Concen_plus_Fluxes.x.numpy()[mask_mets,:].sum(axis=1)  >=2e-6   \n",
    "                                                                                #)\n",
    "                                                                                ]).__len__() > 1\n",
    "\n",
    "assert np.unique(\n",
    "                pyg_graph_Concen_plus_Fluxes.x.numpy()[mask_rxns,:].sum(axis=1)[\n",
    "                                                                                #np.invert(\n",
    "                                                                                    pyg_graph_Concen_plus_Fluxes.x.numpy()[mask_rxns,:].sum(axis=1)  >=2e-6   \n",
    "                                                                                #)\n",
    "                                                                                ]).__len__() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pyg_graph_onlyConcen, \"./results/graphs/PYG_graph_only_Concen.pt\")\n",
    "torch.save(pyg_graph_onlyFluxes, \"./results/graphs/PYG_graph_only_Fluxes.pt\")\n",
    "torch.save(pyg_graph_Concen_plus_Fluxes, \"./results/graphs/PYG_graph_Concen_plus_Fluxes.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ccb92c3caf64d15d8cccade25008a463e602c087926676408820d80b4769698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
