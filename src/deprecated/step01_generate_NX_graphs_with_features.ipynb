{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling...\n",
      " A: min|aij| =  1.000e+00  max|aij| =  1.000e+00  ratio =  1.000e+00\n",
      "Problem data seem to be well scaled\n"
     ]
    }
   ],
   "source": [
    "from custom_clss_and_fncs import *\n",
    "from cobra.io import load_json_model\n",
    "\n",
    "model                                 = load_json_model(\"./COBRA_models/GEM_Recon2_thermocurated_redHUMAN.json\")\n",
    "feature_data                          = pd.read_parquet(\"./results/data/oversampled_augmented_metabolite_data.parquet.gzip\")#\n",
    "feature_names                         = pd.read_csv(\"./results/data/metabolite_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add concentratios as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo_nx                              = cobra_a_networkx(model)\n",
    "\n",
    "node_list = list(grafo_nx.nodes)\n",
    "feature_data.rename(\n",
    "    columns=feature_names.set_index(\"Simbolo_traductor\")[\"Recon3_ID\"].to_dict(), \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "number_to_fill_in = int(1)\n",
    "\n",
    "feature_data[\n",
    "    [item for item in node_list if item not in feature_data.columns.tolist()]\n",
    "] = number_to_fill_in\n",
    "\n",
    "# Re-empaca todo este vector como un diccionario de atributos\n",
    "feature_dict = feature_data.to_dict(orient=\"list\")  # desde Pandas\n",
    "feature_dict = {key: {\"x\": feature_dict[key]} for key in feature_dict}\n",
    "\n",
    "nx.set_node_attributes(grafo_nx, feature_dict)\n",
    "\n",
    "assert feature_data['phe_L_c'].tolist() == grafo_nx.nodes['phe_L_c']['x']\n",
    "assert np.unique(grafo_nx.nodes(data=True)['r0399']['x']) == number_to_fill_in\n",
    "assert len(grafo_nx.nodes(data=True)['r0399']['x']) == len(grafo_nx.nodes(data=True)['phe_L_c']['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph       = from_networkx(copy.deepcopy(nx.Graph(grafo_nx)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attribute     = nx.get_node_attributes(grafo_nx, \"x\")\n",
    "longest_feature = max(len(v) for k,v in x_attribute.items())\n",
    "assert pyg_graph.x.shape[1]  == pyg_graph.num_features == longest_feature # == flux_samples.shape[0]\n",
    "target_node = 'phe_L_c'\n",
    "producto_idx = list(grafo_nx.nodes()).index(target_node)\n",
    "producto_features = grafo_nx.nodes()[target_node]['x']\n",
    "\n",
    "\n",
    "#assert grafo_nx.nodes[target_node]['x'] == flux_dict[target_node]\n",
    "assert np.allclose(pyg_graph.x[producto_idx,:].numpy()[0:producto_features.__len__()], np.array(producto_features), 1e-7, 1e-10)\n",
    "#assert np.allclose(flux_samples[target_node].tolist(), pyg_graph.x[producto_idx,:].numpy()[0:producto_features.__len__()], 1e-7, 1e-10)\n",
    "assert not pyg_graph.is_directed()\n",
    "assert not pyg_graph.has_isolated_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_metabolite_data         = pd.read_parquet(\"./results/data/oversampled_augmented_metabolite_data.parquet.gzip\")#\n",
    "labels                              = torch.tensor(oversampled_metabolite_data.label).reshape(len(oversampled_metabolite_data.label),1)\n",
    "pyg_graph.y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(pyg_graph, \"./results/graphs_from_PYG_and_NX/PYG_graph_only_CONCEN.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "producto_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "lista_de_grafos = []\n",
    "for i in range(pyg_graph.x.shape[1]):\n",
    "   \n",
    "    \n",
    "    new_pyg_data  = Data(x =  pyg_graph.x[:,i].reshape(12437, 1),  y = pyg_graph.y[i], \n",
    "                         edge_index = pyg_graph.edge_index)\n",
    "    new_pyg_data.num_classes = int(2)\n",
    "    lista_de_grafos.append(new_pyg_data)\n",
    "    \n",
    "    \n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def make_loader(graphs: list, batch_size: int  =1*32, num_samples = 500):#batch_size: int  =8*32):\n",
    "\n",
    "    sampler_train_set = RandomSampler(\n",
    "        graphs,\n",
    "        #num_samples= num_samples, #params[\"training\"][\"sampler_num_samples\"],  # Genera un muestreo del grafo\n",
    "        replacement=True,  # con repeticion de muestras\n",
    "    )\n",
    "    return DataLoader(graphs, batch_size=batch_size, sampler = sampler_train_set,  drop_last=True)\n",
    "\n",
    "\n",
    "loader   = make_loader(lista_de_grafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.9550, Test Acc: 0.9634\n",
      "Epoch: 002, Train Acc: 0.8896, Test Acc: 0.8888\n",
      "Epoch: 003, Train Acc: 0.9667, Test Acc: 0.9692\n",
      "Epoch: 004, Train Acc: 0.9530, Test Acc: 0.9531\n",
      "Epoch: 005, Train Acc: 0.9451, Test Acc: 0.9457\n",
      "Epoch: 006, Train Acc: 0.9011, Test Acc: 0.9050\n",
      "Epoch: 007, Train Acc: 0.9601, Test Acc: 0.9616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m train()\n\u001b[1;32m     71\u001b[0m train_acc \u001b[39m=\u001b[39m test(loader)\n\u001b[0;32m---> 72\u001b[0m test_acc \u001b[39m=\u001b[39m test(loader)\n\u001b[1;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [18], line 61\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     59\u001b[0m model\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 61\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:  \u001b[39m# Iterate in batches over the training/test dataset.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     data\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)  \n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:19\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[1;32m     20\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[1;32m     21\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch_geometric/data/batch.py:68\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     58\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     59\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     60\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[1;32m     69\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m     70\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[1;32m     71\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     72\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[1;32m     73\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[1;32m     74\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[1;32m     78\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch_geometric/data/collate.py:84\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m value, slices, incs \u001b[39m=\u001b[39m _collate(attr, values, data_list, stores,\n\u001b[1;32m     85\u001b[0m                                increment)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor) \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mis_cuda:\n\u001b[1;32m     88\u001b[0m     device \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[0;32m/DeepenData/.miniconda/envs/geo/lib/python3.9/site-packages/torch_geometric/data/collate.py:150\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(values, dim\u001b[39m=\u001b[39;49mcat_dim \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m value, slices, incs\n\u001b[1;32m    153\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, SparseTensor) \u001b[39mand\u001b[39;00m increment:\n\u001b[1;32m    154\u001b[0m     \u001b[39m# Concatenate a list of `SparseTensor` along the `cat_dim`.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39m# NOTE: `cat_dim` may return a tuple to allow for diagonal stacking.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GIN\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.GIN_layers =  GIN(in_channels= 1, hidden_channels= hidden_channels, num_layers= 4, \n",
    "                               out_channels= hidden_channels, dropout=0.1,  jk=None, \n",
    "                               act='LeakyReLU', act_first = True)   \n",
    "        \n",
    "        #self.conv1 = GCNConv(1, hidden_channels)\n",
    "        #self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        #self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 2, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        #x = self.conv1(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        x = self.GIN_layers(x, edge_index)\n",
    "        #x = F.dropout(x, p=0.1)\n",
    "        x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) #x\n",
    "    \n",
    "model    = GCN(hidden_channels=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "def train():\n",
    "    model.to('cuda')\n",
    "    model.train()\n",
    "\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "         data.to('cuda')\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     model.to('cuda')\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data.to('cuda')\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    train_acc = test(loader)\n",
    "    test_acc = test(loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(grafo_nx, f\"./results/graphs_from_PYG_and_NX/NX_only_concentrations.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo_nx_v2 = copy.deepcopy(grafo_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add fluxes as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_samples = pd.read_parquet( \"./results/data/flux_samples.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_length = len(grafo_nx.nodes(data=True)['r0399']['x']) \n",
    "\n",
    "\n",
    "flux_dict = flux_samples.sample(feature_length, replace=True).to_dict(orient = 'list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attribute     = nx.get_node_attributes(grafo_nx_v2, \"x\")\n",
    "\n",
    "x_attribute.update(flux_dict)\n",
    "\n",
    "nx.set_node_attributes(grafo_nx_v2, x_attribute, 'x')\n",
    "\n",
    "\n",
    "assert grafo_nx_v2.nodes(data=True)['r0399']['x'] == flux_dict['r0399']\n",
    "assert len(grafo_nx_v2.nodes(data=True)['r0399']['x']) == len(grafo_nx_v2.nodes(data=True)['phe_L_c']['x'])\n",
    "assert grafo_nx_v2.nodes(data=True)['phe_L_c']['x']  == feature_data['phe_L_c'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(grafo_nx_v2, f\"./results/graphs_from_PYG_and_NX/NX_fluxes_and_concentrations.gpickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ccb92c3caf64d15d8cccade25008a463e602c087926676408820d80b4769698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
